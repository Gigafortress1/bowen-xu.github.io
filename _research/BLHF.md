---
title: "Bandit Learning with Human Feedback (BLHF)"
excerpt: "Using human feedback to train the bandit learning mode"
venue: "Using human feedback to train the bandit learning mode"
date: 2023.3 - Present
collection: research
---
**Key words:** Bandit Learning, Reinforcement Learning, Human feedback, Lightweight RLHF.

This project designed a model framework: **Bandit Learning from Human Feedback (BLHF)**, the main role is to use human feedback to train the bandit learning mode. The idea comes from the core technology used by the chatgpt model: Reinforcement Learning from Human Feedback (RLHF).

The basic structure of learning from human feedback is to make the model learn or fine-tune the model according to the results of human feedback. This technology can be mainly used in the training of language models (including chatgpt, etc.), or in decision-making systems (RLHF for decision making)

The motivation of the project comes from the hope that the RLHF based on the large model can be lightweight and use a smaller but more efficient bandit model, so that the new BLHF model can use a smaller amount of data and computing resources than the original RLHF model. To achieve a better learning effect. The purpose of this project is to optimize the model structure and promote the development of small models so that more researchers without large amounts of data and computing resources can train their own models.

My main division of labor in this project is to investigate existing similar models, design model frameworks, and simulations.
