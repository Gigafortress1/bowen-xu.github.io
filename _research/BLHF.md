---
title: "Bandit Learning with Human Feedback (BLHF)"
excerpt: "Using human feedback to train the bandit learning model, 2023.4 - Present"
venue: "Using human feedback to train the bandit learning model, 2023.4 - Present"
date: 2023.3 - Present
collection: research
---
**Key words:** Bandit Learning, Reinforcement Learning, Human feedback, Lightweight RLHF.

This project introduces a model framework called "**Bandit Learning from Human Feedback (BLHF)"**, which uses human feedback to train the bandit learning model. The idea for this framework was inspired by the core technology used in the ChatGPT model: Reinforcement Learning from Human Feedback (RLHF).

The basic structure of learning from human feedback involves making the model learn or fine-tune based on the results of human feedback. This technology is useful in training language models (including ChatGPT) and decision-making systems (such as RLHF for decision making).

The motivation behind this project is to create a lighter-weight alternative to RLHF, using a smaller but more efficient bandit model. By doing so, the new BLHF model can achieve effective learning results while using fewer data and computing resources. The goal of this project is to optimize the model structure and promote the development of smaller models, so that researchers without access to large amounts of data or computing resources can still train their own models effectively.

My main division of labor in this project are:

* Related papers and models in related fields research.
* Model frameworks Design (in process).
* Model simulations (in process).
